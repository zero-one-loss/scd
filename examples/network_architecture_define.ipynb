{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for generating architecture code for your need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because scd optimization code need some specfic flag in the model defination, traditional pytorch model defination could not work here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "## 1. Generate network architecture code \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, you should edit the `main/arch_generate.py`. Here we will generate a traditional MLP with two hidden layer with 20 nodes in each, Relu activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% mdsdfsdf\n"
    }
   },
   "outputs": [],
   "source": [
    "name = 'mlp2rr' # model class name\n",
    "\n",
    "# CIFAR10 flatten vectors' shape is 50000 x 3072\n",
    "structure = [\n",
    "\n",
    "    Layer(layer_type='nn.Linear', in_channel=3072,\n",
    "          out_channel=20, bias=True, act='relu', scale=False),\n",
    "    Layer(layer_type='nn.Linear', in_channel=20,\n",
    "          out_channel=20, bias=True, act='relu', scale=False),\n",
    "    Layer(layer_type='nn.Linear', in_channel=20,\n",
    "          out_channel='num_classes', bias=True, act='relu'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After run `arch_generate.py`, some code will be inserted into `core/cnn01.py` and `core/ensemble_ensemble.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will be added into `core/cnn01.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp2rr(nn.Module):\n",
    "    def __init__(self, num_classes=2, act=\"sign\", sigmoid=False, softmax=False, scale=1, bias=True):\n",
    "        super(mlp2rr, self).__init__()\n",
    "        if act == \"sign\":\n",
    "            self.act = torch.sign\n",
    "        elif act == \"signb\":\n",
    "            self.act = signb\n",
    "        elif act == \"sigmoid\":\n",
    "            self.act = torch.sigmoid_\n",
    "        elif act == \"relu\":\n",
    "            self.act = torch.relu_\n",
    "\n",
    "        if softmax:\n",
    "            if num_classes < 2:\n",
    "                raise ValueError(\"num_classes expect larger than 1, but got {num_classes}\")\n",
    "            self.signb = softmax_\n",
    "        else:\n",
    "            self.signb = torch.sigmoid if sigmoid else signb\n",
    "\n",
    "        self.fc1_si = nn.Linear(3072, 20, bias=bias)\n",
    "        self.fc2_si = nn.Linear(20, 20, bias=bias)\n",
    "        self.fc3_si = nn.Linear(20, num_classes, bias=bias)\n",
    "        self.layers = [\"fc1_si\", \"fc2_si\", \"fc3_si\"]\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def forward(self, x, input_=None, layer=None):\n",
    "        # check input start from which layer\n",
    "        status = -1\n",
    "        for items in self.layers:\n",
    "            status += 1\n",
    "            if input_ is None or items in input_:\n",
    "                break\n",
    "\n",
    "        # layer 1\n",
    "        if status < 1:\n",
    "            if input_ != self.layers[0] + \"_ap\":\n",
    "                out = self.fc1_si(x)\n",
    "            if layer == self.layers[0] + \"_projection\":\n",
    "                return out\n",
    "            if input_ == self.layers[0] + \"_ap\":\n",
    "                out = x\n",
    "            out = torch.relu_(out)\n",
    "            if layer == self.layers[0] + \"_output\":\n",
    "                return out\n",
    "\n",
    "        # layer 2\n",
    "        if input_ == self.layers[1]:\n",
    "            out = x\n",
    "        if status < 2:\n",
    "            if input_ != self.layers[1] + \"_ap\":\n",
    "                out = self.fc2_si(out)\n",
    "            if layer == self.layers[1] + \"_projection\":\n",
    "                return out\n",
    "            if input_ == self.layers[1] + \"_ap\":\n",
    "                out = x\n",
    "            out = torch.relu_(out)\n",
    "            if layer == self.layers[1] + \"_output\":\n",
    "                return out\n",
    "\n",
    "        # layer 3\n",
    "        if input_ == self.layers[2]:\n",
    "            out = x\n",
    "        if status < 3:\n",
    "            if input_ != self.layers[2] + \"_ap\":\n",
    "                out = self.fc3_si(out)\n",
    "            if layer == self.layers[2] + \"_projection\":\n",
    "                return out\n",
    "            if input_ == self.layers[2] + \"_ap\":\n",
    "                out = x\n",
    "            out = self.signb(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "arch['mlp2rr'] = mlp2rr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will be added into `core/ensemble_model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp2rr(nn.Module):\n",
    "    def __init__(self, num_classes=2, act=\"sign\", sigmoid=False, softmax=False, scale=1, votes=1, bias=True):\n",
    "        super(mlp2rr, self).__init__()\n",
    "        self.votes = votes\n",
    "        self.num_classes = num_classes\n",
    "        if act == \"sign\":\n",
    "            self.act = torch.sign\n",
    "        elif act == \"signb\":\n",
    "            self.act = signb\n",
    "        elif act == \"sigmoid\":\n",
    "            self.act = torch.sigmoid_\n",
    "        elif act == \"relu\":\n",
    "            self.act = torch.relu_\n",
    "\n",
    "        if softmax:\n",
    "            if num_classes < 3:\n",
    "                raise ValueError(\"num_classes expect larger than 3, but got {num_classes}\")\n",
    "            self.signb = softmax_\n",
    "        else:\n",
    "            self.signb = torch.sigmoid if sigmoid else signb\n",
    "\n",
    "        self.fc1_si = nn.Conv1d(1, 20 * votes, kernel_size=3072, bias=bias)\n",
    "        self.fc2_si = nn.Conv1d(votes, 20 * votes, kernel_size=20, bias=bias, groups=votes)\n",
    "        self.fc3_si = nn.Conv1d(votes, num_classes * votes, kernel_size=20, bias=bias, groups=votes)\n",
    "        self.layers = [\"fc1_si\", \"fc2_si\", \"fc3_si\"]\n",
    "\n",
    "    def forward(self, out):\n",
    "        out.unsqueeze_(dim=1)\n",
    "        out = self.fc1_si(out)\n",
    "        out = torch.relu_(out)\n",
    "        out = out.reshape((out.size(0), self.votes, -1))\n",
    "        out = self.fc2_si(out)\n",
    "        out = torch.relu_(out)\n",
    "        out = out.reshape((out.size(0), self.votes, -1))\n",
    "        out = self.fc3_si(out)\n",
    "        out = out.reshape((out.size(0), self.votes, self.num_classes))\n",
    "        if self.num_classes == 1:\n",
    "            out = self.signb(out).squeeze(dim=-1)\n",
    "            out = out.mean(dim=1).round()\n",
    "        else:\n",
    "            out = self.signb(out)\n",
    "            out = out.mean(dim=1).argmax(dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "arch['mlp2rr'] = mlp2rr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get a mlp01, with one sign activated hidden layer, you can do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'mlp01scale' # model class name\n",
    "\n",
    "# CIFAR10 flatten vectors' shape is 50000 x 3072\n",
    "structure = [\n",
    "\n",
    "    Layer(layer_type='nn.Linear', in_channel=3072,\n",
    "          out_channel=20, bias=True, act='msign', scale=True), # msign is a sign activation, please set scale to True if activation is sign\n",
    "    Layer(layer_type='nn.Linear', in_channel=20,\n",
    "          out_channel='num_classes', bias=True, act='relu'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you will get similar code as mlp2rr in `core/cnn01.py` and `core/ensemble_model.py`. Then you can do this to import this architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from core.cnn01 import arch\n",
    "\n",
    "net = arch['mlp01scale'](num_classes=1, act='sign', sigmoid=True,\n",
    "    softmax=False, scale=1, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us build a network with three convolution layer with sign activation, and one fully connected layer with relu activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'toy3sss100scale'\n",
    "\n",
    "structure = [\n",
    "\n",
    "    Layer(layer_type='nn.Conv2d', in_channel=3,\n",
    "          out_channel=16, kernel_size=3, padding=1, bias=True,\n",
    "           pool_size=2, act='msign', pool_type='avg', scale=True), # if you dont want to do downsampling, remove pool_size and pool_type\n",
    "    Layer(layer_type='nn.Conv2d', in_channel=16,\n",
    "          out_channel=32, kernel_size=3, padding=1, bias=True,\n",
    "          pool_size=2, act='msign', pool_type='avg', scale=True),\n",
    "    Layer(layer_type='nn.Conv2d', in_channel=32,\n",
    "          out_channel=64, kernel_size=3, padding=1, bias=True,\n",
    "          reshape=True, pool_size=2, act='msign',\n",
    "          pool_type='avg', scale=True), # be carefull, if the next layer is a fully connected layer, set reshape=True to flatten the feature.\n",
    "    Layer(layer_type='nn.Linear', in_channel=64 * 4 * 4,\n",
    "          out_channel=100, bias=True, act='relu', scale=False), # Relu does not need scale, set it to False\n",
    "    Layer(layer_type='nn.Linear', in_channel=100,\n",
    "          out_channel='num_classes', bias=True, act='relu'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you will get this in `core/cnn01.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toy3sss100scale(nn.Module):\n",
    "    def __init__(self, num_classes=2, act=sign, sigmoid=False, softmax=False, scale=1, bias=True):\n",
    "        super(toy3sss100scale, self).__init__()\n",
    "        if act == \"sign\":\n",
    "            self.act = msign\n",
    "        elif act == \"signb\":\n",
    "            self.act = signb\n",
    "        elif act == \"sigmoid\":\n",
    "            self.act = torch.sigmoid_\n",
    "        elif act == \"relu\":\n",
    "            self.act = torch.relu_\n",
    "\n",
    "        if softmax:\n",
    "            if num_classes < 2:\n",
    "                raise ValueError(\"num_classes expect larger than 1, but got {num_classes}\")\n",
    "            self.signb = softmax_\n",
    "        else:\n",
    "            self.signb = torch.sigmoid if sigmoid else signb\n",
    "\n",
    "        self.conv1_si = nn.Conv2d(3, 16, kernel_size=3, padding=1, bias=bias)\n",
    "        self.conv2_si = nn.Conv2d(16, 32, kernel_size=3, padding=1, bias=bias)\n",
    "        self.conv3_si = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=bias)\n",
    "        self.fc4_si = nn.Linear(1024, 100, bias=bias)\n",
    "        self.fc5_si = nn.Linear(100, num_classes, bias=bias)\n",
    "        self.layers = [\"conv1_si\", \"conv2_si\", \"conv3_si\", \"fc4_si\", \"fc5_si\"]\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def forward(self, x, input_=None, layer=None):\n",
    "        # check input start from which layer\n",
    "        status = -1\n",
    "        for items in self.layers:\n",
    "            status += 1\n",
    "            if input_ is None or items in input_:\n",
    "                break\n",
    "\n",
    "        # layer 1\n",
    "        if status < 1:\n",
    "            if input_ != self.layers[0] + \"_ap\":\n",
    "                out = self.conv1_si(x)\n",
    "            if layer == self.layers[0] + \"_projection\":\n",
    "                return out\n",
    "            if input_ == self.layers[0] + \"_ap\":\n",
    "                out = x\n",
    "            out = msign(out) * 0.0833\n",
    "            out = F.avg_pool2d(out, 2)\n",
    "            if layer == self.layers[0] + \"_output\":\n",
    "                return out\n",
    "\n",
    "        # layer 2\n",
    "        if input_ == self.layers[1]:\n",
    "            out = x\n",
    "        if status < 2:\n",
    "            if input_ != self.layers[1] + \"_ap\":\n",
    "                out = self.conv2_si(out)\n",
    "            if layer == self.layers[1] + \"_projection\":\n",
    "                return out\n",
    "            if input_ == self.layers[1] + \"_ap\":\n",
    "                out = x\n",
    "            out = msign(out) * 0.0589\n",
    "            out = F.avg_pool2d(out, 2)\n",
    "            if layer == self.layers[1] + \"_output\":\n",
    "                return out\n",
    "\n",
    "        # layer 3\n",
    "        if input_ == self.layers[2]:\n",
    "            out = x\n",
    "        if status < 3:\n",
    "            if input_ != self.layers[2] + \"_ap\":\n",
    "                out = self.conv3_si(out)\n",
    "            if layer == self.layers[2] + \"_projection\":\n",
    "                return out\n",
    "            if input_ == self.layers[2] + \"_ap\":\n",
    "                out = x\n",
    "            out = msign(out) * 0.0417\n",
    "            out = F.avg_pool2d(out, 2)\n",
    "            out = out.reshape(out.size(0), -1)\n",
    "            if layer == self.layers[2] + \"_output\":\n",
    "                return out\n",
    "\n",
    "        # layer 4\n",
    "        if input_ == self.layers[3]:\n",
    "            out = x\n",
    "        if status < 4:\n",
    "            if input_ != self.layers[3] + \"_ap\":\n",
    "                out = self.fc4_si(out)\n",
    "            if layer == self.layers[3] + \"_projection\":\n",
    "                return out\n",
    "            if input_ == self.layers[3] + \"_ap\":\n",
    "                out = x\n",
    "            out = torch.relu_(out)\n",
    "            if layer == self.layers[3] + \"_output\":\n",
    "                return out\n",
    "\n",
    "        # layer 5\n",
    "        if input_ == self.layers[4]:\n",
    "            out = x\n",
    "        if status < 5:\n",
    "            if input_ != self.layers[4] + \"_ap\":\n",
    "                out = self.fc5_si(out)\n",
    "            if layer == self.layers[4] + \"_projection\":\n",
    "                return out\n",
    "            if input_ == self.layers[4] + \"_ap\":\n",
    "                out = x\n",
    "            out = self.signb(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this in `core/ensemble_model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toy3sss100scale(nn.Module):\n",
    "    def __init__(self, num_classes=2, act=sign, sigmoid=False, softmax=False, scale=1, votes=1, bias=True):\n",
    "        super(toy3sss100scale, self).__init__()\n",
    "        self.votes = votes\n",
    "        self.num_classes = num_classes\n",
    "        if act == \"sign\":\n",
    "            self.act = msign\n",
    "        elif act == \"signb\":\n",
    "            self.act = signb\n",
    "        elif act == \"sigmoid\":\n",
    "            self.act = torch.sigmoid_\n",
    "        elif act == \"relu\":\n",
    "            self.act = torch.relu_\n",
    "\n",
    "        if softmax:\n",
    "            if num_classes < 2:\n",
    "                raise ValueError(\"num_classes expect larger than 3, but got {num_classes}\")\n",
    "            self.signb = softmax_\n",
    "        else:\n",
    "            self.signb = torch.sigmoid if sigmoid else signb\n",
    "\n",
    "        self.conv1_si = nn.Conv2d(3, 16 * votes, kernel_size=3, padding=1, bias=bias)\n",
    "        self.conv2_si = nn.Conv2d(16 * votes, 32 * votes, kernel_size=3, padding=1, bias=bias, groups=votes)\n",
    "        self.conv3_si = nn.Conv2d(32 * votes, 64 * votes, kernel_size=3, padding=1, bias=bias, groups=votes)\n",
    "        self.fc4_si = nn.Conv1d(votes, 100 * votes, kernel_size=1024, bias=bias, groups=votes)\n",
    "        self.fc5_si = nn.Conv1d(votes, num_classes * votes, kernel_size=100, bias=bias, groups=votes)\n",
    "        self.layers = [\"conv1_si\", \"conv2_si\", \"conv3_si\", \"fc4_si\", \"fc5_si\"]\n",
    "\n",
    "    def forward(self, out):\n",
    "        out = self.conv1_si(out)\n",
    "        out = msign(out) * 0.0833\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = self.conv2_si(out)\n",
    "        out = msign(out) * 0.0589\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = self.conv3_si(out)\n",
    "        out = msign(out) * 0.0417\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = out.reshape((out.size(0), self.votes, -1))\n",
    "        out = self.fc4_si(out)\n",
    "        out = torch.relu_(out)\n",
    "        out = out.reshape((out.size(0), self.votes, -1))\n",
    "        out = self.fc5_si(out)\n",
    "        out = out.reshape((out.size(0), self.votes, self.num_classes))\n",
    "        if self.num_classes == 1:\n",
    "            out = self.signb(out).squeeze(dim=-1)\n",
    "            out = out.mean(dim=1).round()\n",
    "        else:\n",
    "            out = self.signb(out)\n",
    "            out = out.mean(dim=1).argmax(dim=-1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do PGD attack, please copy the code above in `core/ensemble_model.py` into `Pytorch_CIFAR10/core/ensemble_model.py` and remove `.round()` and `.argmax(dim=-1)` which will give you **hard predicted label** rather than **probability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-class classification, you have to enable softmax like this. If you don't need bias, you can set **bias=False**. When you combine votes in the future, please add **--no_bias 1** in the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from core.cnn01 import arch\n",
    "\n",
    "net = arch['toy3sss100scale'](num_classes=10, act='sign', sigmoid=False,\n",
    "    softmax=True, scale=1, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can go to next step, train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (scd)",
   "language": "python",
   "name": "pycharm-ae4fd5d8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
